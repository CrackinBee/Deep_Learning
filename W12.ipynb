{"cells":[{"cell_type":"markdown","metadata":{"id":"Qzdhc9nYvMpE"},"source":["# Imports\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"KkZhNR0ILCbk","executionInfo":{"status":"ok","timestamp":1712927234345,"user_tz":-120,"elapsed":12555,"user":{"displayName":"Jolle Verhoog","userId":"07365770099172720712"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9b0fb435-16f7-4dca-b3b1-a773294863af"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_shape_as_tensor' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_reshape_from_tensor' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::reshape' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n","  warnings.warn(\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import cv2\n","import numpy as np\n","\n","from torchsummary import summary\n","import torch.optim as optim\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","from torchvision import transforms\n","import os\n","import matplotlib.pyplot as plt\n","import time\n","from tqdm.notebook import tqdm\n","from torchvision.models import vgg16\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","from torch.optim.lr_scheduler import StepLR\n","from torch.optim import lr_scheduler\n","import random\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"1bcR3b2G3lUG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712865349411,"user_tz":-120,"elapsed":20814,"user":{"displayName":"Mitchell Maassen van den Brink","userId":"09451179875751977301"}},"outputId":"6cfd7d45-94ab-492b-e5e0-02d425d17bc1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"SaNP7sIyuiTU"},"source":["# Modules\n","\n","MFEF: tensor --> tensor\n","\n","Up en downsamplen veranderd naar juiste dimensies"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"OrKQpRizLKZ8","executionInfo":{"status":"ok","timestamp":1712927238433,"user_tz":-120,"elapsed":254,"user":{"displayName":"Jolle Verhoog","userId":"07365770099172720712"}}},"outputs":[],"source":["class REMModule(nn.Module):\n","    def __init__(self, in_channels, hidden_channels):\n","        \"\"\"\n","        Args:\n","            in_channels: number of features of the input image\n","            hidden_channels: list of two numbers which are number of hidden features\n","            out_features: number of features in output layer\n","        \"\"\"\n","        super(REMModule, self).__init__()\n","        # convolutional layers\n","\n","        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1)\n","        self.conv3 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1)\n","        self.conv4 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1)\n","        self.conv5 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1)\n","        self.conv6 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1)\n","        self.conv7 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1)\n","        self.conv8 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1)\n","\n","        # GELU activation functions\n","        self.gelu1 = nn.GELU()\n","        self.gelu2 = nn.GELU()\n","        self.gelu3 = nn.GELU()\n","        self.gelu4 = nn.GELU()\n","        self.gelu5 = nn.GELU()\n","        self.gelu6 = nn.GELU()\n","\n","    def forward(self, x):\n","        print('REM in: ', x.shape) if shape_debug else None\n","        cache1 = x\n","        x = self.conv1(x)\n","        x = self.gelu1(x)\n","        x = self.conv2(x)\n","        x = self.gelu2(x)\n","        x = self.conv3(x)\n","        x = self.gelu3(x)\n","        x = self.conv4(x)\n","        x += cache1\n","        cache2 = x\n","        x = self.conv5(x)\n","        x = self.gelu4(x)\n","        x = self.conv6(x)\n","        x = self.gelu5(x)\n","        x = self.conv7(x)\n","        x = self.gelu6(x)\n","        x = self.conv8(x)\n","        x += cache2\n","\n","        print('REM OUT: ', x.shape) if shape_debug else None\n","        return x\n","\n","\n","class PCAMModule(nn.Module):\n","    def __init__(self, in_channels, hidden_channels):\n","        \"\"\"\n","        Initialize the block.\n","        Args:\n","            in_channels: number of input channels.\n","            hidden_channels: number of channels in the hidden layers.\n","            num_classes: number of output classes.\n","        \"\"\"\n","        super(PCAMModule, self).__init__()\n","\n","        hidden_channels = 1\n","\n","        # Main pathway 1\n","        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=3, padding=1)\n","        self.gelu1 = nn.GELU()\n","\n","        self.conv2 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1)\n","\n","        # Side path 1\n","        self.conv3 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size=1, padding=0)\n","        self.sigmoid1 = nn.Sigmoid()\n","\n","        # Main pathway 2\n","\n","        self.gap = nn.AdaptiveAvgPool2d(1)\n","        self.fc1 = nn.Linear(hidden_channels, hidden_channels)\n","        self.gelu = nn.GELU()\n","        self.fc2 = nn.Linear(hidden_channels, hidden_channels)\n","        self.sigmoid2 = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        print('PCAM in: ', x.shape) if shape_debug else None\n","        x1 = self.gelu1(self.conv1(x))\n","\n","        x2 = self.conv2(x1) * self.sigmoid1(self.conv3(x1))\n","\n","        x3 = x1 * self.sigmoid2(self.fc2(self.gelu(self.fc1(self.gap(x2)))))\n","        x3 = x + x2 + x3        # waarom is dit er als 'x' gereturnd wordt?\n","\n","        print('PCAM OUT: ', x.shape) if shape_debug else None\n","        return x3\n","\n","\n","class MFFModule(nn.Module):\n","    def __init__(self, in_channels = 3, out_channels = 3):\n","        super(MFFModule, self).__init__()\n","        self.out_channels = in_channels #Defined like this in paper idk why\n","\n","        #Layers with weights, biases\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size =1, stride =1, padding = 0)\n","        self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size =3, stride =1, padding = 1)\n","        self.conv5 = nn.Conv2d(in_channels, out_channels, kernel_size =5, stride =1, padding = 2)\n","        #Convolution dimension depends on way of concatenation #TODO: Check this dimension\n","        self.conv3_1 = nn.Conv2d(3*in_channels, out_channels, kernel_size =3, stride =1, padding = 1)\n","        #Layers without weights, biases\n","        self.GeLu = nn.GELU()\n","\n","\n","    def forward(self, FB1, FB2, FB3):\n","        print('MFF in: ', FB1.shape, FB2.shape, FB3.shape) if shape_debug else None\n","\n","        cache_1 = FB1\n","        cache_2 = FB2\n","        cache_3 = FB3\n","\n","        FB1 = self.GeLu(self.conv1(FB1))\n","        FB2 = self.GeLu(self.conv3(FB2))\n","        FB3 = self.GeLu(self.conv5(FB3))\n","\n","        FB1 += cache_1\n","        FB2 += cache_2\n","        FB3 += cache_3\n","        #CS = nn.ChannelShuffle(self.out_channels)\n","        X = self.conv3_1(torch.cat((FB1, FB2, FB3), dim = 0))\n","\n","        print('MFF out: ', X.shape) if shape_debug else None\n","        return X\n","\n","\n","class MFEFModule(nn.Module):\n","  def __init__(self):\n","    super(MFEFModule, self).__init__()\n","\n","    self.REM1 = REMModule(3,3)\n","    self.REM2 = REMModule(3,3)\n","    self.REM3 = REMModule(3,3)\n","    self.REM4 = REMModule(3,3)\n","    self.REM5 = REMModule(6,6)\n","    self.REM6 = REMModule(9,9)\n","    self.REM7 = REMModule(18,18)\n","    self.REM8 = REMModule(36,36)\n","    self.REM9 = REMModule(54,54)\n","\n","    self.MFF1 = MFFModule()\n","    self.MFF2 = MFFModule()\n","    self.MFF3 = MFFModule()\n","\n","    self.PCAM1 = PCAMModule(18, 1)\n","    self.PCAM2 = PCAMModule(18, 1)\n","    self.PCAM3 = PCAMModule(18, 1)\n","    self.conv_layer = nn.Conv2d(54, 3, kernel_size=3, padding=1)\n","\n","\n","  def forward(self, img_tensor, wb_tensor, clahe_tensor):\n","    FB1 = self.REM1.forward(img_tensor)\n","    FB2 = self.REM2.forward(wb_tensor)\n","    FB3 = self.REM3.forward(clahe_tensor)\n","\n","    MFF_1 = self.MFF1.forward(FB1, FB2, FB3)\n","    MFF_2 = self.MFF2.forward(FB1, FB2, FB3)\n","    MFF_3 = self.MFF3.forward(FB1, FB2, FB3)\n","\n","    REM_3 = self.REM4.forward(MFF_3)\n","    REM_2 = self.REM5.forward(torch.concat((MFF_2, REM_3)))\n","    REM_1 = self.REM6.forward(torch.concat((MFF_1, REM_2)))\n","\n","    chan1 = torch.add(REM_1, torch.concat((img_tensor, img_tensor, img_tensor)))\n","    chan2 = torch.add(REM_2, torch.concat((wb_tensor,wb_tensor)))\n","    chan3 = torch.add(REM_3, clahe_tensor)\n","\n","    chan1_2 = torch.nn.functional.interpolate(chan1.unsqueeze(0), scale_factor=0.5, mode='area').squeeze(0)\n","    chan2_2 = torch.nn.functional.interpolate(chan2.unsqueeze(0), scale_factor=0.5, mode='area').squeeze(0)\n","    chan3_2 = torch.nn.functional.interpolate(chan3.unsqueeze(0), scale_factor=0.5, mode='area').squeeze(0)\n","    print(\"First downsample channels: \", chan1_2.shape, chan2_2.shape, chan3_2.shape) if shape_debug else None\n","\n","    chan1_4 = torch.nn.functional.interpolate(chan1_2.unsqueeze(0), scale_factor=0.5, mode='area').squeeze(0)\n","    chan2_4 = torch.nn.functional.interpolate(chan2_2.unsqueeze(0), scale_factor=0.5, mode='area').squeeze(0)\n","    chan3_4 = torch.nn.functional.interpolate(chan3_2.unsqueeze(0), scale_factor=0.5, mode='area').squeeze(0)\n","    print(\"Second downsample channels: \", chan1_4.shape, chan2_4.shape, chan3_4.shape) if shape_debug else None\n","\n","    chan_big = torch.concat((chan1, chan2, chan3))\n","    chan_mid = torch.concat((chan1_2, chan2_2, chan3_2))\n","    chan_sml = torch.concat((chan1_4, chan2_4, chan3_4))\n","\n","    pcam_big = self.PCAM1.forward(chan_big)\n","    pcam_mid = self.PCAM2.forward(chan_mid)\n","    pcam_sml = self.PCAM3.forward(chan_sml)\n","\n","    pcam_sml_rem = self.REM7.forward(pcam_sml)\n","    pcam_sml_rem_up = torch.nn.functional.interpolate(pcam_sml_rem.unsqueeze(0), scale_factor=2, mode='area').squeeze(0)\n","    print(\"Shape pcam_sml_rem_up: \", pcam_sml_rem_up.shape) if shape_debug else None\n","\n","    pcam_mid_rem = self.REM8.forward(torch.concat((pcam_mid, pcam_sml_rem_up)))\n","    pcam_mid_rem_up = torch.nn.functional.interpolate(pcam_mid_rem.unsqueeze(0), scale_factor=2, mode='area').squeeze(0)\n","    print(\"Shape pcam_mid_rem_up: \", pcam_mid_rem_up.shape) if shape_debug else None\n","\n","    pcam_big_rem = self.REM9.forward(torch.concat((pcam_big, pcam_mid_rem_up)))\n","\n","    final_img = self.conv_layer(pcam_big_rem)\n","\n","    return final_img\n","\n","class PerceptualLoss(nn.Module):\n","    def __init__(self, device):\n","        super(PerceptualLoss, self).__init__()\n","        self.vgg = vgg16(weights='DEFAULT').features[:23].eval()\n","        self.vgg = self.vgg.to(device)\n","        for param in self.vgg.parameters():\n","            param.requires_grad = False\n","\n","    def forward(self, output, target):\n","        vgg_output = self.vgg(output)\n","        vgg_target = self.vgg(target)\n","        return torch.nn.functional.l1_loss(vgg_output, vgg_target)"]},{"cell_type":"markdown","metadata":{"id":"iqy6rp1Mupp8"},"source":["# Functies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5C8nFHWPueS8"},"outputs":[],"source":["def apply_white_balance(img, alpha=0.2):\n","    img4 = img.copy()\n","    avg_b = np.mean(img4[:, :, 0])\n","    avg_g = np.mean(img4[:, :, 1])\n","    avg_r = np.mean(img4[:, :, 2])\n","\n","    # lighting estimate mu\n","    mu = (avg_b + avg_g + avg_r) / 3\n","    mu_prime = 0.5 + alpha * mu\n","\n","    # Scale the image channels\n","    img4[:, :, 0] = np.clip(img4[:, :, 0] * (mu_prime / avg_b), 0, 255)\n","    img4[:, :, 1] = np.clip(img4[:, :, 1] * (mu_prime / avg_g), 0, 255)\n","    img4[:, :, 2] = np.clip(img4[:, :, 2] * (mu_prime / avg_r), 0, 255)\n","\n","    return img4\n","\n","\n","def apply_clahe(img, clip_limit=2.0, tile_grid_size=(8, 8)):\n","    img3 = img.copy()\n","    lab = cv2.cvtColor(img3, cv2.COLOR_BGR2LAB)\n","    l, a, b = cv2.split(lab)\n","\n","    # CLAHE to the L channel\n","    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n","    cl = clahe.apply(l)\n","\n","    merged_lab = cv2.merge((cl, a, b))\n","    final_img = cv2.cvtColor(merged_lab, cv2.COLOR_LAB2BGR)\n","    final_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","    return final_img\n","\n","\n","def preprocess_image(img_path, decrease=False):\n","    img = cv2.imread(img_path)\n","    if img is None:\n","        return None\n","    img = resize(img)\n","    if decrease:\n","        img = cv2.resize(img, (720, 720))\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    return img\n","\n","\n","def resize(img):\n","    height, width, _ = img.shape\n","    new_height = height - height % 12\n","    new_width = width - width % 12\n","    return cv2.resize(img, (new_width, new_height))\n","\n","\n","def create_tensor_datasets(input_folder, reference_folder, verbose=False):\n","  # sorted list of file names\n","  file_names = sorted(f for f in os.listdir(input_folder) if f.endswith('.png'))\n","  convert_tensor = transforms.ToTensor()\n","\n","  raw_orig_data = []\n","  raw_wb_data = []\n","  raw_clahe_data = []\n","  reference_data =[]\n","\n","  for file_name in file_names:\n","      raw_path = os.path.join(input_folder, file_name)\n","      reference_path = os.path.join(reference_folder, file_name)\n","\n","      # Preprocess images\n","      raw_img = preprocess_image(raw_path, decrease=decrease)\n","      reference_img = preprocess_image(reference_path, decrease=decrease)\n","\n","      if raw_img is not None and reference_img is not None:\n","          raw_orig_data.append(convert_tensor(raw_img))\n","          raw_wb_data.append(convert_tensor(apply_white_balance(raw_img)))\n","          raw_clahe_data.append(convert_tensor(apply_clahe(raw_img)))\n","          reference_data.append(convert_tensor(reference_img))\n","      else:\n","          print(f\"Error loading images for {file_name}. Skipping...\")\n","\n","  if verbose:\n","    print(\"Amount of images loaded: \", len(raw_orig_data))\n","\n","  raw_orig_dataset = torch.stack(raw_orig_data)\n","  raw_wb_dataset = torch.stack(raw_wb_data)\n","  raw_clahe_dataset = torch.stack(raw_clahe_data)\n","  reference_tensor_dataset = torch.stack(reference_data)\n","\n","  return raw_orig_dataset, raw_wb_dataset, raw_clahe_dataset, reference_tensor_dataset\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jz_KJpg4y6Hp"},"source":["# Settings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hxeS36UBRu_Q","outputId":"b035f83a-04ad-44a3-d282-bc8b155f3cb0","executionInfo":{"status":"ok","timestamp":1712850501998,"user_tz":-120,"elapsed":819,"user":{"displayName":"Jolle Verhoog","userId":"07365770099172720712"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Amount of images loaded:  10\n"]}],"source":["# Settings\n","decrease = True\n","shape_debug = False\n","\n","input_folder = \"/content/raw\"           # raw folder aanmaken in colab er daarin je foto's zetten\n","reference_folder = \"/content/reference\"     # raw folder aanmaken in colab er daarin je foto's zetten\n","\n","# Check for GPU availability\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')\n","\n","# Datasets laden\n","raw_orig_dataset, raw_wb_dataset, raw_clahe_dataset, reference_tensor_dataset = create_tensor_datasets(input_folder, reference_folder, verbose=True)"]},{"cell_type":"markdown","metadata":{"id":"uLrEHFehyzlY"},"source":["# Training werkend in colab zonder aanpassing:\n","Deze beter niet aanpassen voor reference"]},{"cell_type":"code","source":["from torchsummary import summary\n","model = MFEFModule()\n","shape_debug = False\n","summary(model, [(3, 16, 16), (3, 16, 16), (3, 16, 16)], device='cpu') # (in_channels, height, width)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"B-nuZN5ebGUn","executionInfo":{"status":"error","timestamp":1712927468527,"user_tz":-120,"elapsed":623,"user":{"displayName":"Jolle Verhoog","userId":"07365770099172720712"}},"outputId":"f92f5196-6d96-450a-81b1-3186476291fe"},"execution_count":11,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Given groups=1, weight of size [3, 9, 3, 3], expected input[6, 3, 16, 16] to have 9 channels, but got 3 channels instead","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-ffdfc4d705d5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMFEFModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mshape_debug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (in_channels, height, width)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-76bd6f9413a0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img_tensor, wb_tensor, clahe_tensor)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0mFB3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREM3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclahe_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mMFF_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMFF1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFB1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFB2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFB3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0mMFF_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMFF2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFB1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFB2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFB3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mMFF_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMFF3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFB1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFB2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFB3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-76bd6f9413a0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, FB1, FB2, FB3)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mFB3\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcache_3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m#CS = nn.ChannelShuffle(self.out_channels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFB1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFB2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFB3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MFF out: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshape_debug\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1561\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1562\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1563\u001b[0m                 for hook_id, hook in (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [3, 9, 3, 3], expected input[6, 3, 16, 16] to have 9 channels, but got 3 channels instead"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HFEcvEvmeECP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712850311180,"user_tz":-120,"elapsed":10986,"user":{"displayName":"Jolle Verhoog","userId":"07365770099172720712"}},"outputId":"a6a61498-194a-4c5f-bca5-f10501a898da"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:07<00:00, 74.2MB/s]\n"]}],"source":["model = MFEFModule().to(device)\n","l1_loss = nn.L1Loss()\n","perceptual_loss = PerceptualLoss(device)\n","optimizer = optim.AdamW(model.parameters(), lr=0.001)\n","scheduler = StepLR(optimizer, step_size=25, gamma=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"executionInfo":{"elapsed":404,"status":"error","timestamp":1712779982194,"user":{"displayName":"Mitchell .Maassen van den Brink","userId":"10916870713712126212"},"user_tz":-120},"id":"Bd3Hx1z4d1Df","outputId":"89382b5b-b3f9-4999-c613-f6903025cd3b"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'raw_orig_dataset' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-ad48435c222d>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mrandom_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_orig_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mzipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_orig_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_wb_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_clahe_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_tensor_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'raw_orig_dataset' is not defined"]}],"source":["epochs = 20         # in de paper is dit 200\n","loss_epochs = []\n","num_indexes =  1    # batch size\n","#decrease = False    # als we echt gaan trainen\n","\n","model.train()\n","for epoch in range(epochs):\n","    start_time = time.time()\n","\n","    random_indexes = random.sample(range(0,len(raw_orig_dataset)), num_indexes)\n","    zipped = zip(raw_orig_dataset[random_indexes], raw_wb_dataset[random_indexes], raw_clahe_dataset[random_indexes], reference_tensor_dataset[random_indexes])\n","\n","    optimizer.zero_grad() # deze er ook buiten ?\n","\n","    for img_tensor, wb_tensor, clahe_tensor, ref_tensor in zipped:\n","        img_tensor, wb_tensor, clahe_tensor, ref_tensor = img_tensor.to(device), wb_tensor.to(device), clahe_tensor.to(device), ref_tensor.to(device)\n","        output = model.forward(img_tensor, wb_tensor, clahe_tensor)\n","        loss = l1_loss(output, ref_tensor) + 0.05 * perceptual_loss(output, ref_tensor)\n","        loss.backward()\n","\n","    optimizer.step()  # Deze update buiten of binnen de vorige loop ?\n","    # scheduler.step()\n","\n","    end_time = time.time()\n","    epoch_time = end_time - start_time\n","\n","    # current_lr = scheduler.get_last_lr()[0]\n","    current_lr = 0.001\n","    print(f'Epoch {epoch+1}, Loss: {loss.item()}, LR, {current_lr}, Duration {epoch_time:.2f} seconds')\n","    loss_epochs.append(loss.item())\n","    # current_lr = scheduler.get_last_lr()[0]\n","\n","torch.save(model.state_dict(), f'model_epoch_{epochs}.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4LVj_8sBg6yC"},"outputs":[],"source":["torch.cuda.empty_cache() # dit als je cache vol is doen"]},{"cell_type":"markdown","metadata":{"id":"ut3-CSuToEdz"},"source":["# Training aangepast\n","Hier heb ik wat dingen geprobeerd, maar geen beter resultaat gekregen. Jullie kunnen hier alles aanpassen wat je wil"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":2825,"status":"error","timestamp":1712850508291,"user":{"displayName":"Jolle Verhoog","userId":"07365770099172720712"},"user_tz":-120},"id":"h15QAkuvoGtE","outputId":"49b2b411-4222-4138-812e-cefe4713e68c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Amount of images loaded:  10\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Given groups=1, weight of size [1, 9, 3, 3], expected input[1, 18, 360, 360] to have 9 channels, but got 18 channels instead","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-8faaa78833c2>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwb_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclahe_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzipped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m           \u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwb_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclahe_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwb_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclahe_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m           \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwb_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclahe_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m           \u001b[0mloss_indiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_tensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.05\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mperceptual_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_indiv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-3045e6da40a3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img_tensor, wb_tensor, clahe_tensor)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mchan_sml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchan1_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchan2_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchan3_4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0mpcam_big\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPCAM1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchan_big\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0mpcam_mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPCAM2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchan_mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mpcam_sml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPCAM3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchan_sml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-3045e6da40a3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PCAM in: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshape_debug\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [1, 9, 3, 3], expected input[1, 18, 360, 360] to have 9 channels, but got 18 channels instead"]}],"source":["model = MFEFModule().to(device)\n","l1_loss = nn.L1Loss()\n","perceptual_loss = PerceptualLoss(device)\n","optimizer = optim.AdamW(model.parameters(), lr=0.001)\n","\n","epochs = 3         # in de paper is dit 200\n","loss_epochs = []\n","num_indexes =  8    # batch size (paper = 16)\n","batches = 5 # hoeveel batches per epoch (paper = 50)\n","\n","input_folder = '/content/raw'\n","reference_folder = '/content/reference'\n","\n","# opnieuw deze functie definieren om nog kleiner te resizen 720 -> 360\n","def preprocess_image(img_path, decrease=False):\n","    img = cv2.imread(img_path)\n","    if img is None:\n","        return None\n","    img = resize(img)\n","    if decrease:\n","        img = cv2.resize(img, (360, 360))\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    return img\n","\n","raw_orig_dataset, raw_wb_dataset, raw_clahe_dataset, reference_tensor_dataset = create_tensor_datasets(input_folder, reference_folder, verbose=True)\n","\n","\n","model.train()\n","for epoch in range(epochs):\n","    for batch in range(batches):\n","      start_time = time.time()\n","      random_indexes = random.sample(range(0,len(raw_orig_dataset)), num_indexes)\n","      zipped = zip(raw_orig_dataset[random_indexes], raw_wb_dataset[random_indexes], raw_clahe_dataset[random_indexes], reference_tensor_dataset[random_indexes])\n","      # zipped is nu de batch\n","      del random_indexes\n","\n","      optimizer.zero_grad()\n","      loss = torch.zeros(1, device=device)\n","\n","      for img_tensor, wb_tensor, clahe_tensor, ref_tensor in zipped:\n","          img_tensor, wb_tensor, clahe_tensor, ref_tensor = img_tensor.to(device), wb_tensor.to(device), clahe_tensor.to(device), ref_tensor.to(device)\n","          output = model(img_tensor, wb_tensor, clahe_tensor)\n","          loss_indiv = l1_loss(output, ref_tensor) + 0.05 * perceptual_loss(output, ref_tensor)\n","          loss += loss_indiv\n","          del loss_indiv, output\n","\n","      loss = loss / num_indexes\n","      loss.backward()\n","      optimizer.step()\n","\n","      end_time = time.time()\n","      epoch_time = end_time - start_time\n","\n","      print(f'Epoch {epoch+1}, Batch {batch+1}, Loss: {loss.item()}, Duration {epoch_time:.2f} seconds')\n","      loss_epochs.append(loss.item())\n","\n","torch.save(model.state_dict(), f'model_epoch_{epochs}_batch_{batches}_batchsize_{num_indexes}.pth')"]},{"cell_type":"markdown","source":["# Laatste versie trainen\n"],"metadata":{"id":"x10nx4fRbzl0"}},{"cell_type":"code","source":["# Check for GPU availability\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')\n","\n","model = MFEFModule().to(device)\n","l1_loss = nn.L1Loss(device)\n","perceptual_loss = PerceptualLoss(device)\n","optimizer = optim.AdamW(model.parameters(), lr=0.001)\n","decrease = True\n","epochs = 200         # in de paper is dit 200\n","train_loss_epochs = []\n","test_loss_epochs = []\n","num_indexes =  16    # batch size (paper = 16)\n","\n","\n","# opnieuw deze functie definieren om nog kleiner te resizen 720 -> 360\n","\n","def preprocess_image(img_path, decrease=True):\n","    img = cv2.imread(img_path)\n","    if img is None:\n","        return None\n","    img = resize(img)\n","    if decrease:\n","        img = cv2.resize(img, (720, 720))\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    return img\n","# Settings\n","decrease = True\n","shape_debug = False\n","\n","input_folder = '/content/drive/My Drive/raw'                   # AANPASSEN VOOR JEZELF\n","reference_folder = '/content/drive/My Drive/reference'         # AANPASSEN VOOR JEZELF\n","\n","# Datasets laden\n","raw_orig_dataset, raw_wb_dataset, raw_clahe_dataset, reference_tensor_dataset = create_tensor_datasets(input_folder, reference_folder, verbose=True)\n","\n","#make train and test split with raw and reference data\n","raw_train, raw_test, ref_train, ref_test = train_test_split(raw_orig_dataset, reference_tensor_dataset, test_size=90/890, random_state=42)\n","wb_train, wb_test, _,  _ = train_test_split(raw_wb_dataset, reference_tensor_dataset, test_size=90/890, random_state=42)\n","clahe_train, clahe_test, _, _ = train_test_split(raw_clahe_dataset, reference_tensor_dataset, test_size=90/890, random_state=42)\n","\n","del raw_orig_dataset, raw_wb_dataset, raw_clahe_dataset, reference_tensor_dataset\n","batches = len(raw_train)/num_indexes # hoeveel batches per epoch (paper = 50)\n","\n","for epoch in range(epochs):\n","    samples_indexes = np.arange(len(raw_train))\n","    random.shuffle(samples_indexes)\n","    batch_indexes = samples_indexes.reshape(int(batches), num_indexes)\n","    for batch, random_indexes in enumerate(batch_indexes):\n","      start_time = time.time()\n","      zipped = zip(raw_train[random_indexes], wb_train[random_indexes], clahe_train[random_indexes], ref_train[random_indexes])\n","      # zipped is nu de batch\n","      del random_indexes\n","\n","      optimizer.zero_grad()\n","      loss = torch.zeros(1, device=device)\n","      grad = torch.zeros(1, device=device)\n","\n","      for img_tensor, wb_tensor, clahe_tensor, ref_tensor in zipped:\n","          img_tensor, wb_tensor, clahe_tensor, ref_tensor = img_tensor.to(device), wb_tensor.to(device), clahe_tensor.to(device), ref_tensor.to(device)\n","          output = model.forward(img_tensor, wb_tensor, clahe_tensor)\n","          loss += l1_loss(output, ref_tensor) + 0.05 * perceptual_loss(output, ref_tensor)\n","          del output\n","\n","      loss = loss / num_indexes\n","      loss.backward()\n","      optimizer.step()\n","\n","      end_time = time.time()\n","      epoch_time = end_time - start_time\n","\n","      print(f'Epoch {epoch+1}, Batch {batch+1}, Loss: {loss.item()}, Duration {epoch_time:.2f} seconds')\n","      train_loss_epochs.append(loss.item())\n","\n","    #compute test loss\n","    #make batches of test to prevent memory errors\n","    batches_test = len(raw_test)//10\n","    test_loss = torch.zeros(1, device=device)\n","    for i in range(batches_test):\n","        indexes = np.arange(i*10, ((i+1)*10)%len(raw_test))\n","        zipped = zip(raw_test[indexes], wb_test[indexes], clahe_test[indexes], ref_test[indexes])\n","        for img_tensor, wb_tensor, clahe_tensor, ref_tensor in zipped:\n","            img_tensor, wb_tensor, clahe_tensor, ref_tensor = img_tensor.to(device), wb_tensor.to(device), clahe_tensor.to(device), ref_tensor.to(device)\n","            test_output = model.forward(img_tensor, wb_tensor, clahe_tensor)\n","            test_loss += l1_loss(test_output, ref_tensor) + 0.05 * perceptual_loss(test_output, ref_tensor)\n","            del test_output\n","    test_loss /= len(raw_test)\n","    test_loss_epochs.append(test_loss.item())\n","    print(f'Epoch {epoch+1}, test Loss: {test_loss.item()}')\n","\n","torch.save(model.state_dict(), f'model_epoch_{epochs}_batch_{batches}_batchsize_{num_indexes}.pth')"],"metadata":{"id":"qIPGijfZb8-S","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ce4c31c1-3d24-4861-ed6b-4c2caa5f6ede"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["Amount of images loaded:  890\n","Epoch 1, Batch 1, Loss: 0.43893298506736755, Duration 50.12 seconds\n","Epoch 1, Batch 2, Loss: 0.28636765480041504, Duration 42.78 seconds\n","Epoch 1, Batch 3, Loss: 0.23201920092105865, Duration 39.26 seconds\n","Epoch 1, Batch 4, Loss: 0.1968494951725006, Duration 36.94 seconds\n","Epoch 1, Batch 5, Loss: 0.15876686573028564, Duration 36.44 seconds\n","Epoch 1, Batch 6, Loss: 0.15775664150714874, Duration 34.71 seconds\n","Epoch 1, Batch 7, Loss: 0.1536044031381607, Duration 35.09 seconds\n","Epoch 1, Batch 8, Loss: 0.1535426676273346, Duration 33.59 seconds\n","Epoch 1, Batch 9, Loss: 0.14140303432941437, Duration 34.57 seconds\n","Epoch 1, Batch 10, Loss: 0.1458745151758194, Duration 35.32 seconds\n","Epoch 1, Batch 11, Loss: 0.14604449272155762, Duration 34.54 seconds\n","Epoch 1, Batch 12, Loss: 0.12332328408956528, Duration 34.87 seconds\n","Epoch 1, Batch 13, Loss: 0.13023778796195984, Duration 34.99 seconds\n","Epoch 1, Batch 14, Loss: 0.16598199307918549, Duration 34.94 seconds\n","Epoch 1, Batch 15, Loss: 0.10943444073200226, Duration 34.83 seconds\n","Epoch 1, Batch 16, Loss: 0.10605955123901367, Duration 34.04 seconds\n","Epoch 1, Batch 17, Loss: 0.13907822966575623, Duration 34.02 seconds\n","Epoch 1, Batch 18, Loss: 0.12077590078115463, Duration 34.08 seconds\n","Epoch 1, Batch 19, Loss: 0.10076095163822174, Duration 33.80 seconds\n","Epoch 1, Batch 20, Loss: 0.13509507477283478, Duration 34.26 seconds\n","Epoch 1, Batch 21, Loss: 0.12461646646261215, Duration 33.77 seconds\n","Epoch 1, Batch 22, Loss: 0.11401169002056122, Duration 33.40 seconds\n","Epoch 1, Batch 23, Loss: 0.12882255017757416, Duration 34.25 seconds\n","Epoch 1, Batch 24, Loss: 0.097186379134655, Duration 34.73 seconds\n","Epoch 1, Batch 25, Loss: 0.11324729025363922, Duration 34.01 seconds\n","Epoch 1, Batch 26, Loss: 0.14164817333221436, Duration 34.50 seconds\n","Epoch 1, Batch 27, Loss: 0.09406764805316925, Duration 35.08 seconds\n","Epoch 1, Batch 28, Loss: 0.11099918186664581, Duration 34.14 seconds\n","Epoch 1, Batch 29, Loss: 0.12021481990814209, Duration 34.91 seconds\n","Epoch 1, Batch 30, Loss: 0.1301063597202301, Duration 33.86 seconds\n","Epoch 1, Batch 31, Loss: 0.12200606614351273, Duration 34.43 seconds\n","Epoch 1, Batch 32, Loss: 0.1153097152709961, Duration 35.06 seconds\n","Epoch 1, Batch 33, Loss: 0.11899441480636597, Duration 33.69 seconds\n","Epoch 1, Batch 34, Loss: 0.11170466989278793, Duration 34.27 seconds\n","Epoch 1, Batch 35, Loss: 0.1128721609711647, Duration 34.47 seconds\n","Epoch 1, Batch 36, Loss: 0.10741151869297028, Duration 34.05 seconds\n","Epoch 1, Batch 37, Loss: 0.11301170289516449, Duration 35.00 seconds\n","Epoch 1, Batch 38, Loss: 0.12854328751564026, Duration 35.40 seconds\n","Epoch 1, Batch 39, Loss: 0.09510151296854019, Duration 34.75 seconds\n","Epoch 1, Batch 40, Loss: 0.12003619223833084, Duration 33.32 seconds\n","Epoch 1, Batch 41, Loss: 0.10768747329711914, Duration 34.59 seconds\n","Epoch 1, Batch 42, Loss: 0.1073257252573967, Duration 34.74 seconds\n","Epoch 1, Batch 43, Loss: 0.11299169063568115, Duration 35.12 seconds\n","Epoch 1, Batch 44, Loss: 0.1119229719042778, Duration 34.60 seconds\n","Epoch 1, Batch 45, Loss: 0.11466237902641296, Duration 34.63 seconds\n","Epoch 1, Batch 46, Loss: 0.10494658350944519, Duration 34.25 seconds\n","Epoch 1, Batch 47, Loss: 0.14554060995578766, Duration 35.32 seconds\n","Epoch 1, Batch 48, Loss: 0.11001238226890564, Duration 36.25 seconds\n","Epoch 1, Batch 49, Loss: 0.10994406789541245, Duration 34.48 seconds\n","Epoch 1, Batch 50, Loss: 0.09776273369789124, Duration 34.44 seconds\n","Epoch 1, test Loss: 0.10318243503570557\n"]}]},{"cell_type":"markdown","source":["# Visualisatie\n"],"metadata":{"id":"Uw4NEYJzcD9N"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":1086,"status":"ok","timestamp":1712767325723,"user":{"displayName":"Jolle Verhoog","userId":"07365770099172720712"},"user_tz":-120},"id":"sRWFFaZD1Pz4","outputId":"13a3139b-d358-4c67-9be3-ae280163586f"},"outputs":[{"data":{"image/png":"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\n","text/plain":["<Figure size 1000x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","\n","epochs = range(1, len(train_loss_epochs) + 1)\n","#ff opletten dat dit dus eigenlijk loss per batch is, niet per epoch\n","# (kunnen nog ff andere plot maken waar we alle batches eerst optellen, is makkelijk te doen ipc)\n","plt.figure(figsize=(10, 6))\n","plt.plot(epochs, train_loss_epochs, marker='o', linestyle='-', color='b', label='Loss per Epoch (train)')\n","#plt.plot(epochs, test_loss_epochs, marker='o', linestyle='-', color='r', label='Loss per Epoch (test)')\n","plt.title('Loss per batch (train)')\n","plt.xlabel('batch')\n","plt.yscale('log')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.grid(True)\n","plt.show()\n","epcohs = range(1, len(test_loss_epochs))\n","plt.figure(figsize=(10, 6))\n","#plt.plot(epochs, train_loss_epochs, marker='o', linestyle='-', color='b', label='Loss per Epoch (train)')\n","plt.plot(epochs, test_loss_epochs, marker='o', linestyle='-', color='r', label='Loss per Epoch (test)')\n","plt.title('Loss per Epoch (test)')\n","plt.xlabel('Epoch')\n","plt.yscale('log')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","source":["# Normaliseren\n","Had geen effect voor mij"],"metadata":{"id":"4l_siPZKDVKO"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"elapsed":12611,"status":"ok","timestamp":1712768804705,"user":{"displayName":"Jolle Verhoog","userId":"07365770099172720712"},"user_tz":-120},"id":"DyEgFypv7Sjr","outputId":"38a8873d-9f19-4111-b86d-38397804b5f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["means:  [123.33504373 118.36575917  89.48706184]\n","stds:  [64.92975802 60.97702477 65.90736228]\n","Normalization complete. Normalized images are saved in /content/ref_norm\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nfor raw_norm:\\nmeans:  [132.24416005 124.67951712  74.47884891]\\nstds:  [58.92333127 54.13693165 62.41253828]\\n\\nfor ref_norm\\nmeans:  [123.33504373 118.36575917  89.48706184]\\nstds:  [64.92975802 60.97702477 65.90736228]\\n'"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["import cv2\n","import numpy as np\n","import os\n","\n","def compute_channelwise_mean_std(source_folder):\n","    # Accumulators for each channel\n","    channel_sums = np.zeros(3)\n","    channel_sums_sq = np.zeros(3)\n","    num_pixels = 0\n","\n","    for filename in os.listdir(source_folder):\n","        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n","            image = cv2.imread(os.path.join(source_folder, filename))\n","            num_pixels += image.shape[0] * image.shape[1]\n","\n","            for i in range(3):  # Loop through channels\n","                channel_data = image[:, :, i].astype('float32')\n","                channel_sums[i] += np.sum(channel_data)\n","                channel_sums_sq[i] += np.sum(np.square(channel_data))\n","\n","    means = channel_sums / num_pixels\n","    stds = np.sqrt(channel_sums_sq / num_pixels - np.square(means))\n","    stds[stds == 0] = 1e-6  # Prevent division by zero\n","\n","    return means, stds\n","\n","def normalize_images(source_folder, target_folder, means, stds):\n","    if not os.path.exists(target_folder):\n","        os.makedirs(target_folder)\n","\n","    for filename in os.listdir(source_folder):\n","        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n","            file_path = os.path.join(source_folder, filename)\n","            image = cv2.imread(file_path).astype('float32')\n","\n","            for i in range(3):  # Loop through channels\n","                # Subtract the mean and divide by the standard deviation\n","                image[:, :, i] = (image[:, :, i] - means[i]) / stds[i]\n","\n","            # Normalize to 0-255\n","            min_val, max_val = image.min(), image.max()\n","            if min_val == max_val:\n","                image.fill(127.5)  # Handle the case where the image has zero variance\n","            else:\n","                image = 255 * (image - min_val) / (max_val - min_val)\n","\n","            image = np.clip(image, 0, 255).astype('uint8')\n","\n","            cv2.imwrite(os.path.join(target_folder, filename), image)\n","\n","    print(\"Normalization complete. Normalized images are saved in\", target_folder)\n","\n","# Example usage\n","source_folder = \"/content/reference\"\n","target_folder = \"/content/ref_norm\"\n","grayscale = False\n","means, stds = compute_channelwise_mean_std(source_folder)\n","print('means: ', means)\n","print('stds: ', stds)\n","means = [132.24416005, 124.67951712,  74.47884891]\n","stds = [58.92333127, 54.13693165, 62.41253828]\n","normalize_images(source_folder, target_folder, means, stds)\n","\n","\n","\"\"\"\n","for raw_norm:\n","means:  [132.24416005 124.67951712  74.47884891]\n","stds:  [58.92333127 54.13693165 62.41253828]\n","\n","for ref_norm\n","means:  [123.33504373 118.36575917  89.48706184]\n","stds:  [64.92975802 60.97702477 65.90736228]\n","\"\"\""]},{"cell_type":"markdown","source":["# Reset van cache"],"metadata":{"id":"cES5TOetDZ_z"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"j99ZRiqkoNz9"},"outputs":[],"source":["torch.cuda.empty_cache() # dit als je cache vol is doen"]},{"cell_type":"markdown","metadata":{"id":"LJEywv9OX0AN"},"source":["# Evaluate and visualise\n","\n","Deze werkt alleen als je een .pth file hebt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1I17sAqUmVk-9fN5xpNOz1b3lap2lVX8t"},"id":"X-VmydpakFdA","outputId":"5862a884-7162-4cb7-828d-155fab6f9e3a"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","# Check for GPU availability\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#print(f'Using device: {device}')\n","\n","input_folder = '/content/raw'\n","reference_folder = '/content/reference'\n","\n","raw_orig_dataset, raw_wb_dataset, raw_clahe_dataset, reference_tensor_dataset = create_tensor_datasets(input_folder, reference_folder, verbose=True)\n","model_eval = MFEFModule().to(device)\n","model_eval.load_state_dict(torch.load('/content/model_epoch_5_batch_10_batchsize_16.pth')) #hier de best getrainde inzetten\n","\n","model_eval.eval()\n","with torch.no_grad():\n","    for img_tensor, wb_tensor, clahe_tensor, ref_tensor in zip(raw_orig_dataset, raw_wb_dataset, raw_clahe_dataset, reference_tensor_dataset):\n","        img_tensor, wb_tensor, clahe_tensor, ref_tensor = img_tensor.to(device), wb_tensor.to(device), clahe_tensor.to(device), ref_tensor.to(device)\n","        output = model_eval(img_tensor, wb_tensor, clahe_tensor)\n","\n","        fig, axes = plt.subplots(1, 3, figsize=(12, 6))\n","        axes[0].imshow(transforms.ToPILImage()(img_tensor))\n","        axes[0].set_title(\"Raw Image\")\n","        axes[0].axis('off')\n","\n","        axes[1].imshow(transforms.ToPILImage()(ref_tensor))\n","        axes[1].set_title(\"Reference Image\")\n","        axes[1].axis('off')\n","\n","        axes[2].imshow(transforms.ToPILImage()(output[[2, 0, 1], :, :]))\n","        axes[2].set_title(\"Output Image\")\n","        axes[2].axis('off')\n","\n","        plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3097,"status":"ok","timestamp":1712865061210,"user":{"displayName":"Mitchell Maassen van den Brink","userId":"09451179875751977301"},"user_tz":-120},"id":"ALmwhhyCMEIl","outputId":"1e626557-e2f7-4878-ca83-fe824a66021c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"]}],"source":["%reset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5169,"status":"ok","timestamp":1712759174281,"user":{"displayName":"Jolle Verhoog","userId":"07365770099172720712"},"user_tz":-120},"id":"bh4ikVCo64XJ","outputId":"9339e15c-9436-4a70-f2e6-e52520e35af5"},"outputs":[{"data":{"text/plain":["([123.31657387292599, 119.02616341500875, 73.45408306597031],\n"," [49.882526548861804, 46.51649407446035, 46.464761191923365])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["import cv2\n","import numpy as np\n","\n","def compute_global_mean_std(image_paths):\n","    \"\"\"\n","    Compute the global mean and standard deviation for a dataset of images.\n","\n","    Parameters:\n","    image_paths (list): A list of paths to the images.\n","\n","    Returns:\n","    tuple: A tuple containing two lists - the global mean and the global standard deviation\n","           for each channel in the BGR format.\n","    \"\"\"\n","    mean_sum = np.zeros(3)  # Initialize sum of means for B, G, R\n","    std_sum = np.zeros(3)   # Initialize sum of stds for B, G, R\n","    n_images = len(image_paths)\n","\n","    for path in image_paths:\n","        image = cv2.imread(path)\n","        if image is not None:\n","            # Compute mean and std for each image and sum up\n","            mean_sum += np.mean(image, axis=(0, 1))\n","            std_sum += np.std(image, axis=(0, 1))\n","\n","    # Calculate global mean and std by dividing by the number of images\n","    global_mean = mean_sum / n_images\n","    global_std = std_sum / n_images\n","\n","    return global_mean.tolist(), global_std.tolist()\n","\n","file_names = sorted(f for f in os.listdir(input_folder) if f.endswith('.png'))\n","all_images = []\n","\n","for file_name in file_names:\n","  raw_path = os.path.join(input_folder, file_name)\n","  reference_path = os.path.join(reference_folder, file_name)\n","  all_images.append(raw_path)\n","  all_images.append(reference_path)\n","\n","compute_global_mean_std(all_images)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1D5P8CzsZt3Z"},"outputs":[],"source":["def normalize_image(image, global_mean=[123.31657387292599, 119.02616341500875, 73.45408306597031], global_std=[49.882526548861804, 46.51649407446035, 46.464761191923365]):\n","    \"\"\"\n","    Normalize a single image.\n","\n","    Parameters:\n","    image (numpy.ndarray): A single image as a numpy array (H, W, C).\n","    global_mean (list or numpy.ndarray): The global mean for each channel.\n","    global_std (list or numpy.ndarray): The global standard deviation for each channel.\n","\n","    Returns:\n","    numpy.ndarray: The normalized image.\n","    \"\"\"\n","    # Ensure the mean and std are arrays to allow broadcasting\n","    global_mean = np.array(global_mean)\n","    global_std = np.array(global_std)\n","    # print('size img b4 norm ', image.shape)\n","    # Normalize the image\n","    normalized_image = (image - global_mean) / global_std\n","    # print('size img after norm ', normalized_image.shape)\n","    return normalized_image"]}],"metadata":{"colab":{"collapsed_sections":["SaNP7sIyuiTU","jz_KJpg4y6Hp","uLrEHFehyzlY","ut3-CSuToEdz","4l_siPZKDVKO"],"provenance":[],"gpuType":"V28"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}